
# ğŸ“Œ Case #3: PostgreSQL â†’ Citus (Distributed PostgreSQL)

**Real companies: GitHub, Cloudflare, Mixpanel**

This migration happens when teams **love PostgreSQL** but hit **horizontal scaling limits**.

> â€œWe donâ€™t want to leave SQL. We want SQL to scale.â€

That sentence alone is interview gold.

---

## 1ï¸âƒ£ Original Setup

### Companies

* **GitHub** â€“ large multi-tenant workloads
* **Cloudflare** â€“ globally distributed metadata
* **Mixpanel** â€“ analytics at massive scale

### Initial Database

**Single-node PostgreSQL (with read replicas)**

### Why PostgreSQL Was Chosen

* Strong ACID guarantees
* Rich indexing
* Complex joins
* Developer productivity
* Mature tooling

### Typical Early Architecture

```
App Servers
   â†“
PostgreSQL Primary
   â†“
Read Replicas
```

This works **beautifully**â€¦ until it doesnâ€™t.

---

## 2ï¸âƒ£ The Breaking Point

### ğŸš¨ Real Scaling Pain

#### 1. Write Bottleneck

* All writes go to **one primary**
* Vertical scaling hits limits
* CPU, I/O, WAL contention

#### 2. Data Growth

* Tables grow into **hundreds of millions / billions of rows**
* Vacuum becomes expensive
* Index bloat increases

#### 3. Multi-Tenant Load

* One noisy tenant affects others
* Hard to isolate workloads

#### 4. Sharding Manually Is Hell

Teams tried:

* App-level sharding
* Multiple Postgres clusters
* Routing logic in code

ğŸ’¥ Result:

* Complex code
* Hard joins
* Operational nightmares

---

## 3ï¸âƒ£ Why Citus?

### What is Citus?

**Citus = PostgreSQL + Distributed Tables**

It shards tables **horizontally** while:

* Keeping SQL
* Keeping PostgreSQL extensions
* Keeping ACID (within shard)

### Key Idea

> â€œDistribute data, not logic.â€

### Why Not Other Options?

| Option             | Rejected Because                 |
| ------------------ | -------------------------------- |
| Cassandra          | Weak joins, eventual consistency |
| DynamoDB           | Vendor lock-in, access patterns  |
| MongoDB            | Same scaling pain, less SQL      |
| App-level sharding | Too complex                      |

### Why Citus Won

* Minimal app changes
* SQL stays SQL
* Transparent sharding
* Postgres ecosystem preserved

---

## 4ï¸âƒ£ Architecture Before vs After

### ğŸ”´ Before (Single-node Postgres)

```
App
 â†“
Postgres Primary
 â†“
Replicas
```

### ğŸŸ¢ After (Citus Cluster)

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
App â†’ Router â†’â”‚ Coordinator â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“               â†“               â†“
 Worker 1        Worker 2        Worker 3
 (Shard A)       (Shard B)       (Shard C)
```

ğŸ§  Coordinator:

* Query planning
* Routing

ğŸ§  Workers:

* Store shards
* Execute queries

---

## 5ï¸âƒ£ Code-Level Changes (Very Important)

### Original PostgreSQL Table

```sql
CREATE TABLE events (
  event_id BIGSERIAL PRIMARY KEY,
  user_id BIGINT,
  event_type TEXT,
  created_at TIMESTAMPTZ
);
```

This table **cannot scale infinitely** on one node.

---

### Citus Distributed Table

```sql
SELECT create_distributed_table(
  'events',
  'user_id'
);
```

ğŸ”¥ Thatâ€™s it.
You choose a **distribution key**.

### Why `user_id`?

* Queries usually filter by user
* Co-locates related rows
* Enables fast joins

---

### Query Before (Same After)

```sql
SELECT count(*)
FROM events
WHERE user_id = 42
AND event_type = 'login';
```

âœ” Still SQL
âœ” Now runs on **one shard**, not all

---

## 6ï¸âƒ£ Migration Strategy (How Real Companies Do It)

### Step-by-Step

1. **Identify shard key**

   * Usually tenant_id / user_id / org_id
2. **Create Citus cluster**
3. **Backfill data**

   ```sql
   INSERT INTO events
   SELECT * FROM events_old;
   ```
4. **Dual writes (optional)**
5. **Switch reads**
6. **Drop old tables**

âš ï¸ Big-bang is risky but possible if downtime allowed.

---

## 7ï¸âƒ£ Hidden Pitfalls (This Is Where People Fail)

| Pitfall            | Reality                              |
| ------------------ | ------------------------------------ |
| Bad shard key      | Cross-shard queries kill performance |
| Global joins       | Expensive                            |
| Reference tables   | Must be replicated                   |
| Unique constraints | Must include shard key               |
| Transactions       | ACID only within shard               |

### Example of a Bad Query

```sql
SELECT *
FROM events e
JOIN users u ON e.user_id = u.id;
```

If `users` isnâ€™t co-located â†’ ğŸš¨ slow.

---

## 8ï¸âƒ£ Final Outcome

### What Improved

âœ” Horizontal write scalability
âœ” Retained SQL & Postgres ecosystem
âœ” Simpler than app-level sharding
âœ” Multi-tenant isolation

### What Got Worse

âœ˜ More moving parts
âœ˜ Requires shard-aware thinking
âœ˜ Some queries need redesign

### When NOT to Use Citus

* Small datasets
* No tenant boundary
* Heavy cross-tenant analytics

---

## ğŸ¯ Interview Preparation Section

### Interview Question

> â€œHow do you scale PostgreSQL horizontally?â€

**Strong Answer**

> â€œBy distributing data using systems like Citus, choosing a good shard key, and co-locating related data to preserve fast joins.â€

### Follow-ups You Should Mention

* Shard key selection
* Reference tables
* Cross-shard query cost
* Transaction boundaries

---

## ğŸ‘¨â€ğŸ’» Production Engineer Lessons

* Scaling databases is **about data modeling**
* Bad shard keys haunt you forever
* Distributed SQL â‰  magic
* SQL is preserved, assumptions are not

---
